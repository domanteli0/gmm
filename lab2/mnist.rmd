# MNIST-based Model

### Dependencies

```{r cache=TRUE}
install.packages('knitr')
install.packages('RCurl')
install.packages('dplyr')
install.packages("ggplot2")
install.packages("blogdown")
install.packages("torch")
install.packages('torchvision')
install.packages('terra')
install.packages('OpenImageR')
install.packages('coro')
install.packages('ggplot2')
```

```{r cache=TRUE}
library('torch')
library('torchvision')
install_torch()
```

### Device setup

```{r}
DEVICE <- "mps" 
# if you want to use a cuda device change `device` to `cuda`, like so:
# DEVICE <- "cuda" 
torch_tensor(1, device = DEVICE)
# torch_tensor(1, device = "cuda")
```

### Download dataset

```{r}
train_ds <- mnist_dataset(
  "train",
  download = TRUE,
  train = TRUE,
  transform = transform_to_tensor
)
```

```{r}
test_ds <- mnist_dataset(
  "test",
  download = TRUE,
  train = FALSE,
  transform = transform_to_tensor
)
```

### DataLoader

```{r}
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE)
test_dl <- dataloader(test_ds, batch_size = 32)
```

### Displaying an image

```{r}
images <- train_dl$.iter()$.next()[[1]][1:32, 1, , ] 
images %>%
  purrr::array_tree(1) %>%
  purrr::map(as.raster) %>%
  purrr::iwalk(~{plot(.x)})
```

```{r}
images[1]$shape
```

```{r}
library('OpenImageR')
par(pty="s")
image(
  rotateFixed(as_array(images[1]), 90),
  col=gray.colors(255, start=0, end=1),
  useRaster = TRUE,
)
```

### The Model

```{r}
net <- nn_module(
  "MNIST-CNN",
  
  initialize = function() {
    # in_channels, out_channels, kernel_size, stride = 1, padding = 0
    self$conv1 <- nn_conv2d(1, 32, 3)
    self$conv2 <- nn_conv2d(32, 64, 3)
    self$dropout1 <- nn_dropout(0.25)
    self$dropout2 <- nn_dropout(0.5)
    self$fc1 <- nn_linear(9216, 128)
    self$fc2 <- nn_linear(128, 10)
  },
  
  forward = function(x) {
    x %>% 
      self$conv1() %>%
      nnf_relu() %>%

      self$conv2() %>%
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      self$dropout1() %>%
      torch_flatten(start_dim = 2) %>%
      self$fc1() %>%
      nnf_relu() %>%
      self$dropout2() %>%
      self$fc2()
  }
)
```

```{r}
model <- net()
model <- model$to(device = DEVICE)
```

```{r}
optimizer <- optim_adam(model$parameters)
# loss <- nnf_cross_entropy(output, b[[2]]$to(device = DEVICE))
```

```{r}
library(coro)

EPOCHS = 5

history_train_acc <- array(numeric(), 0)
history_train_loss <- array(numeric(), 0)


for (epoch in 1:EPOCHS) {
  model$train()

  train_loss_values <- c()
  train_total <- 0
  train_correct <- 0

  coro::loop(for (b in train_dl) {
    inputs <- b[[1]]$to(device = DEVICE)
    labels <- b[[2]]$to(device = DEVICE)
    train_total <- train_total + labels$size(1)

    # make sure each batch's gradient updates are calculated from a fresh start
    optimizer$zero_grad()
    # get model predictions
    output <- model(inputs)

    max <- torch_max(output, dim=2)
    preds <- max[[2]] # i.e. index of max value

    # calculate loss
    loss <- nnf_cross_entropy(output, labels)

    train_correct <- train_correct + as_array(torch_sum(preds == labels))

    # track losses
    train_loss_values <- c(train_loss_values, loss$item())

    # calculate gradient
    loss$backward()
    # apply weight updates
    optimizer$step()
  })

  model$eval()

  epoch_loss <- mean(train_loss_values)
  epoch_acc = train_correct / train_total
  history_train_acc  <- c(history_train_acc, epoch_acc)
  history_train_loss <- c(history_train_loss, epoch_loss)

  cat(sprintf("EPOCH: %d/%d\n", epoch, EPOCHS))
  cat(sprintf("  Loss %3f\n", mean(l)))
  cat(sprintf("  Loss %3f\n", epoch_loss))
  cat(sprintf("  Acc  %3f\n", epoch_acc))
  cat(sprintf("\n"))
}
```

```{r}
model$eval()

history_test_loss <- c()
test_total <- 0
test_correct <- 0

coro::loop(for (b in test_dl) {
  inputs <- b[[1]]$to(device = DEVICE)
  labels <- b[[2]]$to(device = DEVICE)

  output <- model(inputs)
  loss <- nnf_cross_entropy(output, labels)
  history_test_loss <- c(history_test_loss, loss$item())
  # torch_max returns a list, with position 1 containing the values 
  # and position 2 containing the respective indices
  predicted <- torch_max(output$data(), dim = 2)[[2]]
  test_total <- test_total + labels$size(1)
  # add number of correct classifications in this batch to the aggregate
  test_correct <- test_correct + (predicted == labels)$sum()$item()
})

mean(history_test_loss) # loss
correct / ttotal # test accuracy
test_total
```

```{r}
library('ggplot2')
par(pty="s")
ggplot(data_, aes(x = 1:EPOCHS, y = history_train_acc)) +
  geom_line(color="blue") + theme(aspect.ratio=0.5)
```

```{r}
par(pty="s")
ggplot(data_, aes(x = 1:EPOCHS, y = history_train_loss)) +
  geom_line() + theme(aspect.ratio=0.5)
```
