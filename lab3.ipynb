{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T20:27:36.049195Z",
     "start_time": "2024-05-14T20:27:34.474766Z"
    }
   },
   "source": [
    "import pathlib\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lab3\n",
    "reload(lab3)\n",
    "\n",
    "import lab3.classes\n",
    "reload(lab3.classes)\n",
    "import lab3.classes as cs\n",
    "\n",
    "import lab3.show\n",
    "reload(lab3.show)\n",
    "from lab3.show import display_img_with_masks, display_masks\n",
    "\n",
    "import lab3.trans\n",
    "reload(lab3.trans)\n",
    "from lab3.trans import validation_trans, train_trans, test_trans\n",
    "\n",
    "import lab3.dataset\n",
    "reload(lab3.dataset)\n",
    "from lab3.dataset import FiftyOneDataset\n",
    "\n",
    "import lab3.net\n",
    "reload(lab3.net)\n",
    "from lab3.net import Net\n",
    "\n",
    "import lab1.device\n",
    "reload(lab1.device)\n",
    "from lab1.device import device\n",
    "\n",
    "import lab3.util\n",
    "reload(lab3.util)\n",
    "from lab3.util import seconds_to_time\n",
    "\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T20:27:36.052235Z",
     "start_time": "2024-05-14T20:27:36.049889Z"
    }
   },
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "import fiftyone as fo\n",
    "import fiftyone.utils.openimages as fouo\n",
    "import fiftyone.zoo as foz\n",
    "import cv2\n",
    "\n",
    "# COCO = {\n",
    "#   \"fo_type\": fo.types.COCODetectionDataset,\n",
    "#   \"torch_ds\": \n",
    "#   \"short_name\": \"coco\",\n",
    "# }\n",
    "# \n",
    "# FORMAT = COOC\n",
    "\n",
    "def download_and_export(split =\"train\", max_samples: int = 2000):\n",
    "  ds =  foz.load_zoo_dataset(\n",
    "    \"open-images-v6\",\n",
    "    split        = split,\n",
    "    label_types  = [\"segmentations\"],\n",
    "    classes      = cs.classes_no_background,\n",
    "    max_samples  = max_samples,\n",
    "    dataset_dir  = \"data-lab3\",\n",
    "    dataset_name =f\"open-images-v6-{split}\"\n",
    "  )\n",
    "  \n",
    "  # Print the schema of the dataset to inspect field names\n",
    "  print(\"FIELD SCHEMA\\n\")\n",
    "  print(ds.get_field_schema())\n",
    "  \n",
    "  # Print the labels of the first sample to see the available fields\n",
    "  # first_sample = ds.first()\n",
    "  # print(first_sample)\n",
    "  \n",
    "  # Possible formats:\n",
    "  #  - COCODetectionDataset   | CocoDetection\n",
    "  #  - VOCDetectionDataset    | VOCSegmentation\n",
    "  ds.export(\n",
    "    export_dir = f\"./data-lab3-export/{split}\",\n",
    "    dataset_type = fo.types.COCODetectionDataset,\n",
    "    # label_field = \"ground_truth\",\n",
    "    label_field = \"segmentations\",\n",
    "    progress = True,\n",
    "  )\n",
    "  \n",
    "  return ds\n",
    "\n",
    "DOWNLOAD = False\n",
    "if DOWNLOAD:\n",
    "  download_and_export(\"train\")\n",
    "  download_and_export(\"validation\", max_samples = 300)\n",
    "  download_and_export(\"test\", max_samples = 300)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T20:27:36.054735Z",
     "start_time": "2024-05-14T20:27:36.053515Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T20:27:36.057511Z",
     "start_time": "2024-05-14T20:27:36.055326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def resize_dataset(path):\n",
    "  for p in os.listdir(path):\n",
    "    print('Img_filename : ', p)\n",
    "    image = cv2.imread(path + p, cv2.IMREAD_UNCHANGED)\n",
    "    # print(image)\n",
    "\n",
    "    if image.shape[1] > image.shape[0]:\n",
    "      scale = 128 / image.shape[0]\n",
    "    else:\n",
    "      scale = 128 / image.shape[1]\n",
    "\n",
    "    width = int(image.shape[1] * scale)\n",
    "    height = int(image.shape[0] * scale)\n",
    "    dim = (width, height)\n",
    "\n",
    "    # resize image\n",
    "    resized = cv2.resize(image, dim, interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "    print('Resized Dimensions : ', dim)\n",
    "    print('Resized Dimensions : ', resized.shape)\n",
    "    break\n",
    "\n",
    "RESIZE = False\n",
    "if RESIZE:\n",
    "  resize_dataset('./data-lab3-export/test/data/')\n",
    "  resize_dataset('./data-lab3-export/validation/data/')\n",
    "  resize_dataset('./data-lab3-export/train/data/')\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T20:29:26.792697Z",
     "start_time": "2024-05-14T20:29:25.557350Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.utils.data as tud\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as mask_utils\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# CocoSegmentationFiltered shamelessly stolen from ChatGPT\n",
    "class CocoSegmentationFiltered(Dataset):\n",
    "    def __init__(self, root, annFile, classes_of_interest, transform):\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "        # Retrieve class IDs for the classes of interest\n",
    "        self.class_ids = self.coco.getCatIds(catNms=classes_of_interest)\n",
    "        self.class_id_to_index = {cat_id: i for i, cat_id in enumerate(self.class_ids)}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        path = os.path.join(self.root, img_info['file_name'])\n",
    "\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id, catIds=self.class_ids, iscrowd=None)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "        masks = np.zeros((\n",
    "            len(self.class_ids),\n",
    "            img_info['height'],\n",
    "            img_info['width']),\n",
    "            dtype=np.uint8)\n",
    "        \n",
    "        for ann in anns:\n",
    "            if 'segmentation' in ann:\n",
    "                cat_id = ann['category_id']\n",
    "                if cat_id in self.class_id_to_index:\n",
    "                    cat_index = self.class_id_to_index[cat_id]\n",
    "                    rle = coco.annToRLE(ann)\n",
    "                    m = mask_utils.decode(rle).astype(np.uint8)\n",
    "                    masks[cat_index] = np.maximum(masks[cat_index], m)\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        \n",
    "        return image, masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Create an instance of the dataset\n",
    "test_ds = CocoSegmentationFiltered(\n",
    "  root=\"data-lab3-export/test/data\",\n",
    "  annFile=\"data-lab3-export/test/labels.json\",\n",
    "  transform = test_trans,\n",
    "  classes_of_interest=cs.classes)\n",
    "\n",
    "# Use DataLoader to load the dataset\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "  test_ds,\n",
    "  batch_size = BATCH_SIZE,\n",
    "  shuffle=False,\n",
    "  collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "# Create an instance of the dataset\n",
    "train_ds = CocoSegmentationFiltered(\n",
    "    root=\"data-lab3-export/train/data\",\n",
    "    annFile=\"data-lab3-export/train/labels.json\",\n",
    "    transform = train_trans,\n",
    "    classes_of_interest=cs.classes)\n",
    "\n",
    "# Use DataLoader to load the dataset\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "def show_image_and_masks(image, masks, classes_of_interest):\n",
    "    image = image.permute(1, 2, 0).numpy() # Convert from (C, H, W) to (H, W, C)\n",
    "    image = image[..., ::-1]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1 + len(classes_of_interest), figsize=(15, 5))\n",
    "    \n",
    "    # Ensure the image has the correct dtype and scale\n",
    "    if image.max() > 1:\n",
    "        image = image / 255.0  # Scale to [0, 1] if the range is [0, 255]\n",
    "\n",
    "    # Show the original image\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    # Show the masks\n",
    "    for i in range(len(classes_of_interest)):\n",
    "        mask = masks[i].numpy()\n",
    "        ax[i + 1].imshow(mask, cmap='gray')\n",
    "        ax[i + 1].set_title(classes_of_interest[i])\n",
    "        ax[i + 1].axis('off')\n",
    "        # TODO: set dimensions\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# num_samples_to_show = 3\n",
    "# for i, (image, masks) in enumerate(test_dl):\n",
    "#     if i >= num_samples_to_show:\n",
    "#         break\n",
    "#     show_image_and_masks(image[0], masks[0], cs.classes)\n",
    "\n",
    "# NUM_WORKERS = 8\n",
    "# BATCH_SIZE  = 256\n",
    "# \n",
    "# train_ds = make_dataset(\"train\", train_trans)\n",
    "# test_ds  = make_dataset(\"test\",  test_trans)\n",
    "# \n",
    "# train_dl =  tud.DataLoader(train_ds,\n",
    "#   num_workers = NUM_WORKERS,\n",
    "#   batch_size = BATCH_SIZE,\n",
    "#   shuffle = True\n",
    "# )\n",
    "# \n",
    "# test_dl =  tud.DataLoader(\n",
    "#   test_ds,\n",
    "#   num_workers = NUM_WORKERS,\n",
    "#   batch_size = BATCH_SIZE,\n",
    "#   shuffle = False\n",
    "# )\n",
    "# \n",
    "\n",
    "\n",
    "# Iterate through the data\n",
    "for imgs, masks in test_dl:\n",
    "    print(masks[0].shape)\n",
    "    # Do something with imgs and masks\n",
    "    break\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "torch.Size([5, 683, 1024])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T20:27:37.066151Z",
     "start_time": "2024-05-14T20:27:37.061778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from statistics import mean\n",
    "from sklearn.metrics import f1_score\n",
    "from torch import tensor as t, concat as c\n",
    "\n",
    "ZERO = torch.zeros(0, 6, 128, 128) < 1\n",
    "\n",
    "class Stats:\n",
    "  length = 0\n",
    "  \n",
    "  dice: float = 0\n",
    "  iou:  float  = 0\n",
    "  f1_micro: float = 0\n",
    "  f1_macro: float = 0\n",
    "  \n",
    "  loss_acum: list[float] = []\n",
    "  \n",
    "  def init(self):\n",
    "    pass\n",
    "  \n",
    "  def add_data(self, ys: torch.Tensor, y_hats: torch.Tensor, loss: float):\n",
    "    # print(f\"TYPE: {type(ys)}\")\n",
    "    # print(f\"LEN:  {ys}\")\n",
    "    self.length += ys.shape[0]\n",
    "    self.loss_acum.append(loss)\n",
    "\n",
    "    intersection = torch.bitwise_and(ys, y_hats)\n",
    "    union = torch.bitwise_or(ys, y_hats)\n",
    "\n",
    "    intersection = torch.sum(intersection)\n",
    "    union = torch.sum(union)\n",
    "\n",
    "    # Jaccard = || A \\intersect B || / || A \\union B ||\n",
    "    iou = intersection / union\n",
    "\n",
    "    # DICE = 2 || A \\intersect B || / (||A|| + ||B||)\n",
    "    dice = 2 * intersection / (torch.sum(ys) + torch.sum(y_hats))\n",
    "\n",
    "    # Flatten the tensors\n",
    "    ys_flat = ys.view(-1).numpy()\n",
    "    y_hats_flat = y_hats.view(-1).numpy()\n",
    "\n",
    "    # Calculate Micro-F1 and Macro-F1 scores\n",
    "    micro_f1 = f1_score(ys_flat, y_hats_flat, average='micro')\n",
    "    macro_f1 = f1_score(ys_flat, y_hats_flat, average='macro')\n",
    "    \n",
    "    self.iou  += iou.item()\n",
    "    self.dice += dice.item()\n",
    "    self.f1_macro += macro_f1.item()\n",
    "    self.f1_micro += micro_f1.item()\n",
    "  \n",
    "  def get_stats(self):\n",
    "    return (\n",
    "      self.iou / self.length, \n",
    "      self.dice / self.length,\n",
    "      self.f1_micro / self.length,\n",
    "      self.f1_macro / self.length,\n",
    "      mean(self.loss_acum)\n",
    "    )\n",
    "\n",
    "def run_epoch(model: Net,\n",
    "              loader: torch.utils.data.DataLoader,\n",
    "              loss_fn, optimizer):\n",
    "  stats = Stats()\n",
    "  IS_TRAIN = optimizer is not None\n",
    "  \n",
    "  if IS_TRAIN:\n",
    "    model.train()\n",
    "  else:\n",
    "    model.eval()\n",
    "\n",
    "  ix = -1\n",
    "  for (i1, i2), true_masks in loader:\n",
    "    print(type(images))\n",
    "    print(type(true_masks))\n",
    "    images = images.to(device)\n",
    "    true_masks = true_masks.to(device)\n",
    "    \n",
    "    if not IS_TRAIN:\n",
    "      with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "    else:\n",
    "      predictions = model(images)\n",
    "\n",
    "    loss = loss_fn(true_masks, predictions)\n",
    "\n",
    "    # print(f\"  PRED_MAX: {predictions.max()}\")\n",
    "    predictions = predictions > 0.5\n",
    "    true_masks = true_masks > 0.5\n",
    "    stats.add_data(true_masks.cpu().detach(), predictions.cpu().detach(), loss.cpu().detach().item())\n",
    "    \n",
    "    if IS_TRAIN:\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "    if ix == 0:\n",
    "      break\n",
    "    ix -= 1\n",
    "  \n",
    "  ret = stats.get_stats()\n",
    "  return ret\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T20:27:37.070004Z",
     "start_time": "2024-05-14T20:27:37.066975Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def train_and_eval(model, train_ld, valid_ld, epoch_count = 10, learning_rate = 1e-3):\n",
    "  loss_func = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "  start_time = datetime.now()\n",
    "\n",
    "  train_loss_acum  = []\n",
    "  train_iou_acum   = []\n",
    "  train_dice_acum  = []\n",
    "  train_micro_acum = []\n",
    "  train_macro_acum = []\n",
    "  valid_loss_acum  = []\n",
    "  valid_iou_acum   = []\n",
    "  valid_dice_acum  = []\n",
    "  valid_micro_acum = []\n",
    "  valid_macro_acum = []\n",
    "  \n",
    "  for epoch in range(epoch_count):\n",
    "    print(f'EPOCH: {epoch + 1} / {epoch_count}')\n",
    "    train_iou, train_dice, train_micro, train_macro, train_loss = run_epoch(model, train_ld, loss_func, optimizer)\n",
    "    \n",
    "    current_time = datetime.now()\n",
    "    elapsed = seconds_to_time((current_time - start_time).total_seconds())\n",
    "    print(f'  train      | Elapsed: {elapsed}')\n",
    "\n",
    "    valid_iou, valid_dice, valid_micro, valid_macro, valid_loss = run_epoch(model, valid_ld, loss_func, optimizer)\n",
    "    \n",
    "    current_time = datetime.now()\n",
    "    elapsed = seconds_to_time((current_time - start_time).total_seconds())\n",
    "    print(f'  valid      | Elapsed: {elapsed}')\n",
    "\n",
    "    train_iou_acum.append(train_iou)\n",
    "    train_dice_acum.append(train_dice)\n",
    "    train_micro_acum.append(train_micro)\n",
    "    train_macro_acum.append(train_macro)\n",
    "    train_loss_acum.append(train_loss)\n",
    "    \n",
    "    valid_iou_acum.append(valid_iou)\n",
    "    valid_dice_acum.append(valid_dice)\n",
    "    valid_micro_acum.append(valid_micro)\n",
    "    valid_macro_acum.append(valid_macro)\n",
    "    valid_loss_acum.append(valid_loss)\n",
    "\n",
    "    print(f'  Training Loss:  {train_loss},  Validation Loss:  {valid_loss}')\n",
    "    print(f'  Training IoU:   {train_iou},   Validation IoU:   {valid_iou}')\n",
    "    print(f'  Training Dice:  {train_dice},  Validation Dice:  {valid_dice}')\n",
    "    print(f'  Training Micro: {train_micro}, Validation Micro: {valid_micro}')\n",
    "    print(f'  Training Macro: {train_macro}, Validation Macro: {valid_macro}')\n",
    "\n",
    "  return train_iou_acum, valid_iou_acum, train_dice_acum, valid_dice_acum, train_micro_acum, valid_micro_acum, train_macro_acum, valid_macro_acum, train_loss_acum, valid_loss_acum"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T20:27:37.072374Z",
     "start_time": "2024-05-14T20:27:37.070729Z"
    }
   },
   "source": [
    "def plot(train, valid, label = \"IoU\"):\n",
    "  plt.clf()\n",
    "  plt.plot(train, 'b', label = f'Training {label}')\n",
    "  plt.plot(valid, 'r', label = f'Validation {label}')\n",
    "  plt.ylim(0.0, 1.0)\n",
    "  plt.legend()\n",
    "  plt.show()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T20:27:37.083796Z",
     "start_time": "2024-05-14T20:27:37.074595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_dataset[0]\n",
    "# np.zeros(4).reshape((2, 2)).shape\n",
    "img, t = test_ds[0]\n",
    "t"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T20:27:40.639737Z",
     "start_time": "2024-05-14T20:27:37.084657Z"
    }
   },
   "source": [
    "model = Net(test_ds[0][0].shape[0], num_classes = cs.num_classes).to(device)\n",
    "print(f'Parameter count: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "train_iou, valid_iou, train_dice, valid_dice, train_micro, valid_micro, train_macro, valid_macro, train_loss, valid_loss = train_and_eval(\n",
    "    model,\n",
    "    train_dl,\n",
    "    test_dl,\n",
    "    epoch_count = EPOCHS,\n",
    "    learning_rate = 1e-3\n",
    "  )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count: 1,928,582\n",
      "EPOCH: 1 / 100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mParameter count: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28msum\u001B[39m(p\u001B[38;5;241m.\u001B[39mnumel()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mp\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mp\u001B[38;5;241m.\u001B[39mrequires_grad)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m EPOCHS \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[0;32m----> 6\u001B[0m train_iou, valid_iou, train_dice, valid_dice, train_micro, valid_micro, train_macro, valid_macro, train_loss, valid_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_eval\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtest_dl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepoch_count\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1e-3\u001B[39;49m\n\u001B[1;32m     12\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 22\u001B[0m, in \u001B[0;36mtrain_and_eval\u001B[0;34m(model, train_ld, valid_ld, epoch_count, learning_rate)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch_count):\n\u001B[1;32m     21\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEPOCH: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m / \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch_count\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 22\u001B[0m   train_iou, train_dice, train_micro, train_macro, train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mrun_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_ld\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m   current_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[1;32m     25\u001B[0m   elapsed \u001B[38;5;241m=\u001B[39m seconds_to_time((current_time \u001B[38;5;241m-\u001B[39m start_time)\u001B[38;5;241m.\u001B[39mtotal_seconds())\n",
      "Cell \u001B[0;32mIn[5], line 72\u001B[0m, in \u001B[0;36mrun_epoch\u001B[0;34m(model, loader, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     69\u001B[0m   model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     71\u001B[0m ix \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 72\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (i1, i2), true_masks \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[1;32m     73\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(images))\n\u001B[1;32m     74\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(true_masks))\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "iter = train_ld.__iter__()\n",
    "img: torch.Tensor; mask: torch.Tensor; prediction: torch.Tensor\n",
    "imgs, masks = next(iter)\n",
    "\n",
    "# prediction = model(img.unsqueeze(0).to(device)).cpu().detach().squeeze(0) img.shape, mask.unique(), mask.max()\n",
    "# for i in range(10):\n",
    "#   print(masks[0].max())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot(train_iou, valid_iou)\n",
    "plot(train_dice, valid_dice, label = \"dice\")\n",
    "plot(train_micro, valid_micro, label = \"micro\")\n",
    "plot(train_macro, valid_macro, label = \"macro\")\n",
    "plot(train_loss, valid_loss, label = \"loss\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.save(model, 'lab3/net_attempt2.pth')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = torch.load('lab3/net_attempt2.pth').to(device)\n",
    "\n",
    "img, true_mask = valid_dataset[40]\n",
    "prediction = model(img.unsqueeze(0).to(device)).cpu().detach().squeeze(0)\n",
    "\n",
    "true_mask  = true_mask > .5\n",
    "prediction = prediction > .5\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize= (7, 7))\n",
    "display_img_with_masks(ax1, img, true_mask)\n",
    "display_img_with_masks(ax2, img, prediction)\n",
    "display_masks(ax3, true_mask)\n",
    "display_masks(ax4, prediction)\n",
    "\n",
    "# (\n",
    "#   torch.max(true_mask),\n",
    "#   torch.max(prediction),\n",
    "# )\n",
    "# (\n",
    "#   torch.count_nonzero(true_mask),\n",
    "# ),(\n",
    "#   torch.count_nonzero(true_mask != .0),\n",
    "#   (true_mask != .0).float()[0,0,0]\n",
    "# )\n",
    "# prediction.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# t1 = t(\n",
    "# # batch\n",
    "# [\n",
    "#   # masks for img 1\n",
    "#   [\n",
    "#     # mask for class 1\n",
    "#     [[.2, .1], [.1, .3]],\n",
    "#     # mask for class 2\n",
    "#     [[.2, .1], [.1, .3]],\n",
    "#     # mask for class 3\n",
    "#     [[.2, .1], [.1, .3]],\n",
    "#   ],\n",
    "#   # masks for img 2\n",
    "#   [\n",
    "#     [[.2, .1], [.1, .5]],\n",
    "#     [[.2, .1], [.1, .0]],\n",
    "#     [[.2, .1], [.1, .3]],\n",
    "#   ],\n",
    "#   # masks for img 3\n",
    "#   [\n",
    "#     [[.2, .1], [.1, .0]],\n",
    "#     [[.2, .1], [.1, .0]],\n",
    "#     [[.2, .1], [.1, .3]],\n",
    "#   ]\n",
    "# ])\n",
    "# \n",
    "# \n",
    "# # values, indices = t1.max(t([0, 2]))\n",
    "# # (\n",
    "# #   t1,\n",
    "# #   values,\n",
    "# #   (values ** (-1)) #.repeat(3) #.reshape((2, 3)) * t1\n",
    "# # )\n",
    "# \n",
    "# BATCH_SIZE = t(t1.shape)[0]\n",
    "# CLASS_NO = t(t1.shape)[1]\n",
    "# DIM = t(t1.shape)[2]\n",
    "# \n",
    "# vals, ix = t1.max(dim = -1)[0].max(dim = -1)[0].max(dim = -1)\n",
    "# vals = vals ** -1\n",
    "# t2 = vals.repeat_interleave(CLASS_NO * DIM * DIM).view((BATCH_SIZE, CLASS_NO, DIM, DIM))\n",
    "# (\n",
    "#   # vals.repeat(CLASS_NO).reshape((CLASS_NO, 2)).transpose(dim0 = 0, dim1 = 1)\n",
    "#     # .repeat((2, 3))\n",
    "#   # vals.repeat(t1.shape)\n",
    "#   t(t1.shape),\n",
    "#   (BATCH_SIZE, CLASS_NO, DIM, DIM),\n",
    "#   # vals.repeat(t(t1.shape)[:-1].flipud().tolist()).shape\n",
    "#   vals,\n",
    "#   BATCH_SIZE * CLASS_NO * DIM * DIM,\n",
    "#   t(t1.shape).prod(),\n",
    "#   t1 * t2,\n",
    "#   # vals.repeat_interleave(BATCH_SIZE * CLASS_NO * DIM * DIM).shape,\n",
    "#     # vals.repeat_interleave(t(t1.shape).tolist()).shape\n",
    "#   # t1 * vals\n",
    "#   # t1 * vals.repeat_interleave(BATCH_SIZE * CLASS_NO * DIM * DIM).view((2, 3, 2, 2, 2)),\n",
    "#   # (t2 / t1)\n",
    "#   # t2 / t1\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
